{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "archaeological-image-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdFRXR3INJifxvZTlzu3+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shawngraham/hist3000/blob/master/archaeological_image_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61zWhwvDdLG",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started \n",
        "We're going to build an archaeological image classifier using Tensorflow, a library of python packages designed to perform machine learning tasks. We are going to train a model to recognize different kinds of Roman pottery. (We are following a modified version of the [Tensorflow for poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) tutorial, using slightly older versions of Tensorflow, but a version that I know _works_).\n",
        "\n",
        "The data we're going to use comes from [Potsherd: Atlas of Roman Pottery](http://potsherd.net/atlas/potsherd). I've already downloaded the dataset, using a bit of python (as described [here](https://bonetrade.github.io/tutorials/tensorflow-for-poets/)). The zip file with those images is available in the course data directory. It's not the full database; rather, I just put together a small set of ten or so different types of pottery, just to keep things manageable. \n",
        "\n",
        "Download the zip file, and then upload it to your google drive account. (If you go to https://drive.google.com, you can click the big `+ New`, select `upload file`.)\n",
        "\n",
        "Don't put it in a subfolder.  \n",
        "\n",
        "## Make sure we've got a GPU running\n",
        "\n",
        "Click on `edit` -> `notebook settings` and change `hardware accelerator` to `GPU`. Then click `save`. Machine learning is extremely heavy in the sense that it requires a lot of computational resources. Running the calculations on a GPU rather than a CPU (your normal processor) can speed up the work significantly. And of course, some code explicitly requires a GPU rather than a CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjmCKq8tEJ5o",
        "colab_type": "text"
      },
      "source": [
        "## Connect your gdrive\n",
        "\n",
        "The snippet below will make your gdrive available to this notebook. Run the block. It will give you a link to go to, which will ask you to authorize access. Say 'yes'. It will then give you a code. Copy that code, then come back to this block and paste it in the entry box. Once you've done that, your gdrive will become available in the `Files` panel at left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRhKIoeADLQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAMi7uTgEf6a",
        "colab_type": "text"
      },
      "source": [
        "## Copy the zip file from your gdrive, and then unzip it\n",
        "\n",
        "The next code block copies the zipped folder you uploaded to your gdrive into this notebook environment, and then unzips the contents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S1a8MywEsn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you have data already on google drive\n",
        "%cp \"/content/drive/My Drive/data.zip\" data.zip\n",
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plhliRlVFF1b",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# Get Tensorflow Set Up\n",
        "\n",
        "The version we're going to use is 1.7. It's not the most recent version, but I know it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvineJ3iFxee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "d573da36-0558-480e-8169-60ebb9248363"
      },
      "source": [
        "! pip install --upgrade \"tensorflow==1.7.*\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.7.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/83/35c3f53129dfc80d65ebbe07ef0575263c3c05cc37f8c713674dcedcea6f/tensorflow-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (48.1MB)\n",
            "\u001b[K     |████████████████████████████████| 48.1MB 63kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (0.8.1)\n",
            "Collecting tensorboard<1.8.0,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.7.*) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*) (1.0.1)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.4MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.7.*) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*) (3.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=54e315c0c6d6b18505e30fe214625f0215d0578f8eb0b7669cf1592a9eaff2ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.7.0 tensorflow-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_I2Bch6GJSX",
        "colab_type": "text"
      },
      "source": [
        "Now we get the codebase that we'll use from the tensorflow for poets repo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llaK08nAF0_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "32ef688d-1e38-43a8-a0fc-e185c5c0f056"
      },
      "source": [
        "!git clone https://github.com/googlecodelabs/tensorflow-for-poets-2\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow-for-poets-2'...\n",
            "remote: Enumerating objects: 426, done.\u001b[K\n",
            "remote: Total 426 (delta 0), reused 0 (delta 0), pack-reused 426\u001b[K\n",
            "Receiving objects: 100% (426/426), 33.97 MiB | 16.89 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RMzEkvfHHBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and we change our working directory:\n",
        "%cd tensorflow-for-poets-2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuQcqZDAHyx9",
        "colab_type": "text"
      },
      "source": [
        "# Retrain the model\n",
        "Now let's do some training so that the computer learns what Roman archaeology looks like. Because our pottery training data - though copious - is still not enough (fewer than 10000 images), we have to modify our command, concerning validation batch size. Otherwise we’ll get an error message. Note also that we’re only training for 500 steps; more steps will generally get better results (but diminishing returns also apply)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kSqDTXOHyiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m scripts.retrain \\\n",
        "  --bottleneck_dir=tf_files/bottlenecks \\\n",
        "  --how_many_training_steps=500 \\\n",
        "  --model_dir=tf_files/models/ \\\n",
        "  --summaries_dir=tf_files/training_summaries/mobilenet_0.50_224 \\\n",
        "  --output_graph=tf_files/retrained_graph.pb \\\n",
        "  --output_labels=tf_files/retrained_labels.txt \\\n",
        "  --architecture mobilenet_0.50_224 \\\n",
        "  --validation_batch_size=-1 \\\n",
        "  --image_dir=../data/ware # this is the location of our data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te9vR6iOI6ur",
        "colab_type": "text"
      },
      "source": [
        "# Test!\n",
        "\n",
        "This is the fun part. In the data I provided, there is a folder called `unknown`. It has some photos of pottery in there. Let's get our model to identify the pottery!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr008qdIHyM2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "81d560ae-e200-4ae1-8c85-6a2b307aa95b"
      },
      "source": [
        "# let's grab a pot that we know is black burnished ware:\n",
        "!curl \"http://www.pottedhistory.co.uk/image/Roman%20Black%20Burnished%20Ware(800x532).jpg\" > bbtest.jpg"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  114k  100  114k    0     0   492k      0 --:--:-- --:--:-- --:--:--  490k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJY2_8-sOgl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m scripts.label_image \\\n",
        "    --graph=tf_files/retrained_graph.pb  \\\n",
        "    --image=bbtest.jpg\n",
        "\n",
        "## there will be some warnings. But at the end, it will tell you which ware it thinks the photo is of.\n",
        "## (If you want to know what kind of pottery the code represents, just add the code to the end of\n",
        "## this url in a new window: http://potsherd.net/atlas/Ware/) eg\n",
        "## http://potsherd.net/atlas/Ware/BB1 will bring up Black Burnished Ware"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr0fNrkEPFiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## If that image was _misclassified_, can you think why that might be?\n",
        "## Try this one:\n",
        "!curl \"http://www.futuremuseum.co.uk/imageGen.ashx?image=%2fmedia%2f3901%2f9083.jpg\" > bbtest2.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjWX3SaTPTeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m scripts.label_image \\\n",
        "    --graph=tf_files/retrained_graph.pb  \\\n",
        "    --image=bbtest2.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEDIqJ5oPZiY",
        "colab_type": "text"
      },
      "source": [
        "## Success?\n",
        "\n",
        "We've certainly built an image classifier, but there are some problems. There are both technical issues, and ethical issues (on the ethical issues, see [this article about using this kind of tech in the context of human remains](https://intarch.ac.uk/journal/issue52/5/toc.html). Tech-wise, the biggest issue is that we're training on a dataset that is just _too small_. For this to _really_ work well enough to the point where we might actually want to use it, we'd need thousands of images. There are shortcuts we could use (image augmentation, where we rotate and otherwise make slightly different versions of our source images, for instance). \n",
        "\n",
        "Archaeologists are using this basic approach (with much sophistication and nuance) for a wide variety of tasks. [In this paper, they're using it to identify archaeological sites from satellite data.](https://www.pnas.org/content/early/2020/07/17/2005583117).\n",
        "\n",
        "## Going Further\n",
        "\n",
        "You could try finding more training data; you could augment the images you do have [with this code](https://colab.research.google.com/drive/1Sxo8mMdmvdzHkNMAcCTxHomHhaCdkIyf?usp=sharing). "
      ]
    }
  ]
}